#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import json
import random
import re
import time
import os
import sys
from dataclasses import dataclass, replace
from datetime import datetime, timedelta
from typing import Any, Dict, Iterable, List, Optional, Tuple
from urllib.parse import parse_qs, urlparse, urljoin

import requests
from bs4 import BeautifulSoup
from bs4.element import Tag
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from dotenv import load_dotenv

# --- rag-ai í´ë”ì˜ NewsKeywordExtractorë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ê²½ë¡œ ì¶”ê°€ ---
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'rag-ai'))
try:
    from news_keyword_extractor import NewsKeywordExtractor
except ImportError:
    print("âŒ 'rag-ai/news_keyword_extractor.py'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    NewsKeywordExtractor = None

# ====================================================================
# ğŸ”‘ ë„¤ì´ë²„ API í‚¤ (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” .env íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°)
# ====================================================================
# ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ í´ë”ì˜ rag-ai/.env íŒŒì¼ì„ ë¡œë“œ
dotenv_path = os.path.join(os.path.dirname(__file__), '..', 'rag-ai', '.env')
if os.path.exists(dotenv_path):
    load_dotenv(dotenv_path=dotenv_path)

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID", "")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_PASSWD", "") # ë³€ìˆ˜ëª… ìˆ˜ì •

# ====================================================================
# ğŸ§© ì„ íƒì (ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ ì‹œ ì—¬ê¸° ìˆ˜ì •)
# ====================================================================
DEFAULT_TITLE_SELECTORS = ("h2.media_end_head_headline", "h1.media_end_head_headline", "meta[property='og:title']")
DEFAULT_BODY_SELECTORS = ("div#newsct_article", "article#dic_area")
DEFAULT_PHOTO_CAPTION_SELECTORS = (
    ".end_photo_caption", ".end_photo_origin", ".end_photo_origin_source", ".photo_viewer_title",
    ".photo_viewer_origin", ".media_end_head_origin_photo", ".media_end_head_origin_image", ".photo_desc",
    ".img_desc", ".figure_caption", "figcaption",
)
IMAGE_CANDIDATE_ATTRS = ("src", "data-src", "data-lazy-src", "data-original")
DEFAULT_META_TIME_SELECTORS = ("meta[property='article:published_time']", "meta[property='og:article:published_time']", "meta[name='ptime']", "meta[name='date']")
DEFAULT_TIME_SELECTORS = ("span.media_end_head_info_datestamp_time", "em.media_end_head_info_datestamp_time")
DEFAULT_TIME_ATTRS = ("data-date-time", "data-true-date-time", "data-txtexp-time", "datetime")

# ====================================================================
# ğŸ§± ë°ì´í„° í´ë˜ìŠ¤
# ====================================================================
@dataclass
class Config:
    keywords: List[str]
    max_articles_per_keyword: int
    total_phases: int
    backend_endpoint: str
    timeout: int
    wait_min: int
    wait_max: int
    sleep_min: float
    sleep_max: float
    user_agent: str
    accept_language: str
    referer: str
    max_retries: int
    loop: bool
    dry_run: bool
    debug_save_page: bool
    article_url: Optional[str] = None

@dataclass(frozen=True)
class DomainProfile:
    name: str
    title_selectors: Tuple[str, ...]
    body_selectors: Tuple[str, ...]
    photo_caption_selectors: Tuple[str, ...] = DEFAULT_PHOTO_CAPTION_SELECTORS
    meta_time_selectors: Tuple[str, ...] = DEFAULT_META_TIME_SELECTORS
    time_selectors: Tuple[str, ...] = DEFAULT_TIME_SELECTORS
    time_attr_candidates: Tuple[str, ...] = DEFAULT_TIME_ATTRS
    publisher_selectors: Tuple[str, ...] = ("meta[property='og:site_name']", "meta[property='og:article:author']")
    referer: Optional[str] = None
    user_agent: Optional[str] = None


GENERIC_PROFILE = DomainProfile(
    name="GENERIC",
    title_selectors=(
        "h1",
        "meta[property='og:title']",
        "title",
    ),
    body_selectors=(
        "article",
        "div.article-body",
        "div#articleBody",
        "div#newsct_article",
    ),
)

NAVER_PROFILE = DomainProfile(
    name="NAVER",
    title_selectors=DEFAULT_TITLE_SELECTORS,
    body_selectors=DEFAULT_BODY_SELECTORS,
    referer="https://news.naver.com/",
)

YONHAP_PROFILE = DomainProfile(
    name="YONHAP",
    title_selectors=(
        "h1.tit",
        "h1.tit-article",
        "div.article-tit h1",
        "meta[property='og:title']",
    ),
    body_selectors=(
        "div.article-body",
        "article.story-news",
        "div.story-news.article",
        "div#articleBody",
        "div.article-txt",
    ),
    photo_caption_selectors=DEFAULT_PHOTO_CAPTION_SELECTORS + (
        "p.figcaption",
        ".photo-caption",
    ),
    meta_time_selectors=DEFAULT_META_TIME_SELECTORS + (
        "meta[property='article:published_time']",
        "meta[name='ptime']",
        "meta[name='pubdate']",
        "meta[name='Date']",
    ),
    time_selectors=(
        "p.update-time",
        "div.article-tit span.txt-time",
        "span.update-time",
        "span.t11",
        "div.write-time",
    ),
    time_attr_candidates=("data-time", "datetime"),
    referer="https://www.yna.co.kr/",
)

JOONGANG_PROFILE = DomainProfile(
    name="JOONGANG",
    title_selectors=(
        "h1#article_title",
        "h1.article_title",
        "div.article_head h1",
        "meta[property='og:title']",
    ),
    body_selectors=(
        "div#article_body",
        "div.article_body",
        "div#article_body_content",
        "section#article_body",
        "article#article-view-content-div",
    ),
    photo_caption_selectors=DEFAULT_PHOTO_CAPTION_SELECTORS + (
        "span.caption",
        "p.caption",
    ),
    meta_time_selectors=DEFAULT_META_TIME_SELECTORS + (
        "meta[name='ptime']",
        "meta[name='datePublished']",
    ),
    time_selectors=(
        "div.article_info span.time",
        "div.article_writer span.time",
        "span.byline",
    ),
    time_attr_candidates=("data-time", "datetime"),
    referer="https://www.joongang.co.kr/",
)

CHOSUN_PROFILE = DomainProfile(
    name="CHOSUN",
    title_selectors=(
        "h1#title_text",
        "h1.news_tit",
        "div.news_title h1",
        "meta[property='og:title']",
    ),
    body_selectors=(
        "div#news_body_id",
        "section#news_body_id",
        "div.article-body",
        "div.article-body-wrap",
    ),
    photo_caption_selectors=DEFAULT_PHOTO_CAPTION_SELECTORS + (
        "em.cap",
        "span.cap",
    ),
    meta_time_selectors=DEFAULT_META_TIME_SELECTORS + (
        "meta[name='datePublished']",
        "meta[name='pubdate']",
    ),
    time_selectors=(
        "div.news_date",
        "p.news_date",
        "span.print_date",
        "div.byline span",
    ),
    time_attr_candidates=("data-time", "datetime"),
    referer="https://www.chosun.com/",
)

ECONOVILL_PROFILE = DomainProfile(
    name="ECONOVILL",
    title_selectors=(
        "header.article-view-header h3.heading",
        "meta[property='og:title']",
    ),
    body_selectors=("article#article-view-content-div",),
)

AMENEWS_PROFILE = DomainProfile(
    name="AMENEWS",
    title_selectors=("meta[property='og:title']", "div.titleWrap strong", "title"),
    body_selectors=("div#viewContent",),
    time_selectors=("div.registModifyDate li span",),
)

WIKITREE_PROFILE = DomainProfile(
    name="WIKITREE",
    title_selectors=("meta[property='og:title']",),
    body_selectors=("div#wikicon",),
    meta_time_selectors=("meta[property='article:published_time']",),
)

PRESS9_PROFILE = DomainProfile(
    name="PRESS9",
    title_selectors=("meta[property='og:title']", "h3.heading"),
    body_selectors=("div[itemprop='articleBody']",),
    meta_time_selectors=("meta[property='article:published_time']",),
)

AJUNEWS_PROFILE = DomainProfile(
    name="AJUNEWS",
    title_selectors=("meta[property='og:title']", "h1.tit_view"),
    body_selectors=("div[itemprop='articleBody']",),
    meta_time_selectors=("meta[property='article:published_time']",),
)

ZIKKIR_PROFILE = DomainProfile(
    name="ZIKKIR",
    title_selectors=("meta[property='og:title']", "h3.heading"),
    body_selectors=("article#article-view-content-div",),
    meta_time_selectors=("meta[property='article:published_time']",),
)

DEFAULT_PROFILE = GENERIC_PROFILE

HOST_PROFILES = {
    "news.naver.com": NAVER_PROFILE,
    "n.news.naver.com": NAVER_PROFILE,
    "media.naver.com": NAVER_PROFILE,
    "www.yna.co.kr": YONHAP_PROFILE,
    "yna.co.kr": YONHAP_PROFILE,
    "m.yna.co.kr": YONHAP_PROFILE,
    "www.joongang.co.kr": JOONGANG_PROFILE,
    "joongang.co.kr": JOONGANG_PROFILE,
    "news.joins.com": JOONGANG_PROFILE,
    "m.joongang.co.kr": JOONGANG_PROFILE,
    "www.chosun.com": CHOSUN_PROFILE,
    "chosun.com": CHOSUN_PROFILE,
    "news.chosun.com": CHOSUN_PROFILE,
    "www.econovill.com": ECONOVILL_PROFILE,
    "amenews.kr": AMENEWS_PROFILE,
    "www.wikitree.co.kr": WIKITREE_PROFILE,
    "www.press9.kr": PRESS9_PROFILE,
    "www.ajunews.com": AJUNEWS_PROFILE,
    "www.ziksir.com": ZIKKIR_PROFILE,
}


def _add_query_param(url: str, key: str, value: str) -> List[str]:
    parsed = urlparse(url)
    existing = parse_qs(parsed.query or "")
    if existing.get(key):
        return url
    new_query = parsed.query + ("&" if parsed.query else "") + f"{key}={value}"
    return parsed._replace(query=new_query).geturl()


def _chosun_variants(url: str) -> List[str]:
    parsed = urlparse(url)
    if "chosun.com" not in parsed.netloc:
        return []
    hosts = [parsed.netloc]
    if parsed.netloc != "www.chosun.com":
        hosts.append("www.chosun.com")
    if parsed.netloc != "news.chosun.com":
        hosts.append("news.chosun.com")
    variants: List[str] = []
    params = (("output", "1"), ("print", "1"), ("svc", "print"), ("view", "print"))
    for host in hosts:
        base = parsed._replace(netloc=host).geturl()
        variants.append(base)
        for key, value in params:
            variants.append(_add_query_param(base, key, value))
    return variants


def _dedupe_preserve(urls: Iterable[str]) -> List[str]:
    seen = set()
    result = []
    for u in urls:
        if not u or u in seen:
            continue
        seen.add(u)
        result.append(u)
    return result


def generate_article_variants(url: str, profile: DomainProfile) -> List[str]:
    variants = [url]
    if profile.name.upper() == "CHOSUN":
        variants.extend(_chosun_variants(url))
    return _dedupe_preserve(variants)


def _normalize_host(host: str) -> str:
    host = host.lower().split(":")[0]
    if host.startswith("www."):
        host = host[4:]
    return host


def resolve_profile(url: Optional[str]) -> DomainProfile:
    if not url:
        return DEFAULT_PROFILE
    host = _normalize_host(urlparse(url).netloc or "")
    for domain, profile in HOST_PROFILES.items():
        if host == _normalize_host(domain) or host.endswith("." + _normalize_host(domain)):
            return profile
    return DEFAULT_PROFILE

# ====================================================================
# ğŸ”§ ìœ í‹¸ ë° íŒŒì‹± í•¨ìˆ˜ë“¤
# ====================================================================

def create_session(cfg: Config) -> requests.Session:
    s = requests.Session()
    s.headers.update({"User-Agent": cfg.user_agent, "Accept-Language": cfg.accept_language, "Referer": cfg.referer})
    retry = Retry(total=cfg.max_retries, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
    s.mount("https://", HTTPAdapter(max_retries=retry))
    s.mount("http://", HTTPAdapter(max_retries=retry))
    return s

def fetch_page(session: requests.Session, url: str, cfg: Config, profile: Optional[DomainProfile] = None) -> requests.Response:
    ua = (profile.user_agent or cfg.user_agent) if profile else cfg.user_agent
    ref = (profile.referer or cfg.referer) if profile else cfg.referer
    session.headers.update({"User-Agent": ua, "Referer": ref})
    time.sleep(random.uniform(cfg.sleep_min, cfg.sleep_max))
    r = session.get(url, timeout=cfg.timeout)
    r.raise_for_status()
    return r

def to_iso(dt: Optional[datetime]) -> Optional[str]:
    if dt is None: return None
    if dt.tzinfo is not None: dt = dt.astimezone().replace(tzinfo=None)
    return dt.replace(microsecond=0).isoformat()

def _parse_datetime_string(raw: Optional[str]) -> Optional[datetime]:
    if not raw: return None
    raw = raw.strip()
    if not raw: return None
    try:
        return datetime.fromisoformat(raw.replace("Z", "+00:00"))
    except ValueError: pass
    text = re.sub(r"(ì…ë ¥|ìˆ˜ì •)\s*", "", raw)
    m = re.search(r"(20\d{2})[.\-](\d{2})[.\-](\d{2}).*?(ì˜¤ì „|ì˜¤í›„)?\s*(\d{1,2}):(\d{2})", text)
    if m:
        year, month, day, period, hour, minute = m.groups()
        hour = int(hour)
        if period == "ì˜¤í›„" and hour != 12: hour += 12
        if period == "ì˜¤ì „" and hour == 12: hour = 0
        return datetime(int(year), int(month), int(day), hour, int(minute))
    return None

def parse_published_at(soup: BeautifulSoup, profile: DomainProfile) -> Optional[datetime]:
    for selector in profile.meta_time_selectors:
        meta = soup.select_one(selector)
        if meta and meta.get("content"):
            dt = _parse_datetime_string(meta.get("content"))
            if dt: return dt
    for selector in profile.time_selectors:
        el = soup.select_one(selector)
        if el:
            for attr in profile.time_attr_candidates:
                if el.get(attr):
                    dt = _parse_datetime_string(el.get(attr))
                    if dt: return dt
            dt = _parse_datetime_string(el.get_text(" ", strip=True))
            if dt: return dt
    return None

def _title_text(el: Optional[Tag]) -> str:
    if el is None: return ""
    return (el.get("content") or "").strip() if el.name == "meta" else el.get_text(strip=True)

def pick_first(soup: BeautifulSoup, selectors: Iterable[str]) -> Optional[Tag]:
    for selector in selectors:
        if el := soup.select_one(selector):
            return el
    return None

def extract_body_text(body_el: Optional[Tag], profile: DomainProfile) -> str:
    if body_el is None: return ""
    for tag in body_el.find_all(["script", "style", "iframe", "div.related-article", "div.ad_wrap", "aside"]): tag.decompose()
    for selector in profile.photo_caption_selectors:
        for caption in body_el.select(selector): caption.decompose()
    for br in body_el.find_all("br"): br.replace_with("\n")
    body = body_el.get_text("\n", strip=True)
    return re.sub(r"\n{3,}", "\n\n", body)

def extract_image_url(soup: BeautifulSoup) -> Optional[str]:
    """ê¸°ì‚¬ì˜ ëŒ€í‘œ ì´ë¯¸ì§€ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # 1ìˆœìœ„: og:image ë©”íƒ€ íƒœê·¸ (ê°€ì¥ ì•ˆì •ì )
    og_image = soup.select_one("meta[property='og:image']")
    if og_image and og_image.get("content"):
        og_url = og_image.get("content")
        # ë„¤ì´ë²„ ê¸°ë³¸ ë¡œê³  ì´ë¯¸ì§€ëŠ” ì œì™¸
        if "pstatic.net/static.news" not in og_url:
            return og_url

    # 2ìˆœìœ„: ë³¸ë¬¸ ë‚´ ì²« ë²ˆì§¸ ì´ë¯¸ì§€
    for selector in ("div#newsct_article img", "article#dic_area img", "div.article-body img"):
        img = soup.select_one(selector)
        if img:
            for attr in IMAGE_CANDIDATE_ATTRS:
                if img.get(attr):
                    return img.get(attr)

    return None

def parse_article_fields(article_url: str, soup: BeautifulSoup, profile: DomainProfile) -> Dict[str, Any]:
    title_el = pick_first(soup, profile.title_selectors)
    body_el = pick_first(soup, profile.body_selectors)
    publisher_el = pick_first(soup, profile.publisher_selectors)

    title = _title_text(title_el)
    body = extract_body_text(body_el, profile)
    publisher = _title_text(publisher_el)
    published_at = parse_published_at(soup, profile)
    image_url = extract_image_url(soup)

    return {"title": title, "body": body, "publisher": publisher, "published_at": published_at, "image_url": image_url}

# ====================================================================
# ğŸšš ë°±ì—”ë“œ ì „ì†¡
# ====================================================================
def send_to_backend(session: requests.Session, cfg: Config, payload: Dict[str, Any]) -> bool:
    if cfg.dry_run:
        print("ğŸ§ª DRY-RUN (POST ìƒëµ):", payload.get("title"))
        return True
    try:
        res = session.post(cfg.backend_endpoint, json=payload, timeout=cfg.timeout)
        if res.status_code >= 400:
            print(f"âŒ ì„œë²„ ì‘ë‹µ ë‚´ìš© (ìƒíƒœ ì½”ë“œ: {res.status_code}):")
            print(res.text)
        res.raise_for_status()
        print(f"ğŸ“¦ ì €ì¥ ì™„ë£Œ: {payload.get('title')}")
        return True
    except requests.exceptions.RequestException as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

# ====================================================================
# ğŸ§  API ê¸°ë°˜ ìˆ˜ì§‘ ë° ì²˜ë¦¬ ë¡œì§
# ====================================================================

def fetch_articles_from_naver_api(session: requests.Session, cfg: Config, keyword: str) -> List[Dict[str, Any]]:
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        print("âŒ ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸ ID ë˜ëŠ” ì‹œí¬ë¦¿ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    print(f"ğŸ” API ê²€ìƒ‰: '{keyword}'")
    headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
    params = {"query": keyword, "display": cfg.max_articles_per_keyword, "start": 1, "sort": "date"}
    try:
        res = session.get("https://openapi.naver.com/v1/search/news.json", headers=headers, params=params, timeout=cfg.timeout)
        res.raise_for_status()
        data = res.json()
        # ë„¤ì´ë²„ ë‰´ìŠ¤ URL(n.news.naver.com)ì´ ìˆëŠ” ê¸°ì‚¬ë§Œ í•„í„°ë§ (HTML êµ¬ì¡° í†µì¼)
        articles = [
            item for item in data.get("items", [])
            if item.get("link") and "n.news.naver.com" in item.get("link", "")
        ]
        print(f"  â¡ï¸ API ì‘ë‹µ: {len(articles)}ê°œ ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ë°œê²¬")
        return articles
    except requests.exceptions.RequestException as e:
        print(f"âŒ API ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []

def process_article(session: requests.Session, cfg: Config, article_api_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    # ë„¤ì´ë²„ ë‰´ìŠ¤ URL ì‚¬ìš© (HTML êµ¬ì¡° í†µì¼)
    link = article_api_data.get("link")
    if not link: return None

    print(f"  C: GET article {link}")
    try:
        page_res = fetch_page(session, link, cfg, resolve_profile(link))
        page_html = page_res.text
        soup = BeautifulSoup(page_html, "lxml")
        
        parsed_data = parse_article_fields(link, soup, resolve_profile(link))
        
        title = parsed_data.get("title") or BeautifulSoup(article_api_data.get("title", ""), "html.parser").get_text(strip=True)
        body = parsed_data.get("body")
        
        if not body:
            print(f"  C ERR: ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {link}")
            return None
        
        payload = {
            "articleUrl": link,
            "title": title,
            "content": body,
            "publisher": parsed_data.get("publisher"),
            "publishedAt": to_iso(parsed_data.get("published_at") or _parse_datetime_string(article_api_data.get("pubDate"))),
            "contentCrawledAt": to_iso(datetime.utcnow()),
            "isFullContentCrawled": True,
            "image_url": parsed_data.get("image_url"),
        }
        
        return {"payload": payload, "html": page_html, "metadata": {}}

    except Exception as e:
        print(f"  C ERR: ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({link}): {e}")
        if cfg.debug_save_page and 'page_html' in locals():
            with open("error_page.html", "w", encoding="utf-8") as f: f.write(page_html)
        return None

def crawl_with_api_2_phase(session: requests.Session, cfg: Config, keyword_extractor: Optional[NewsKeywordExtractor]):
    processed_urls = set()
    articles_for_phase2 = []

    print("\n--- ğŸŸ¢ 1ë‹¨ê³„ ìˆ˜ì§‘ ì‹œì‘ ---")
    for keyword in cfg.keywords:
        for article_api_data in fetch_articles_from_naver_api(session, cfg, keyword):
            url = article_api_data.get("link")
            if not url or url in processed_urls: continue
            processed_urls.add(url)
            
            result = process_article(session, cfg, article_api_data)
            if result:
                send_to_backend(session, cfg, result["payload"])
                if keyword_extractor:
                    articles_for_phase2.append(result)
        time.sleep(1)

    if not keyword_extractor or cfg.total_phases < 2 or not articles_for_phase2:
        print("\n--- âœ… ìˆ˜ì§‘ ì™„ë£Œ (1ë‹¨ê³„ë¡œ ì¢…ë£Œ) ---")
        return

    print("\n--- ğŸŸ¡ 2ë‹¨ê³„ ìˆ˜ì§‘ ì‹œì‘ ---")
    phase2_keywords = set()
    print("   í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    for article_data in articles_for_phase2:
        keywords = keyword_extractor.extract_keywords(article_data["html"], n=5, metadata=article_data["metadata"])
        for kw, score in keywords:
            if score > 0.3: phase2_keywords.add(kw)
    
    unique_new_keywords = list(phase2_keywords - set(cfg.keywords))
    if not unique_new_keywords:
        print("  ì¶”ê°€ì ì¸ í•µì‹¬ í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("\n--- âœ… ìˆ˜ì§‘ ì™„ë£Œ (2ë‹¨ê³„ ìƒëµ) ---")
        return

    print(f"  2ë‹¨ê³„ ìˆ˜ì§‘ì„ ìœ„í•œ ì‹ ê·œ í‚¤ì›Œë“œ: {unique_new_keywords}")
    for keyword in unique_new_keywords:
        for article_api_data in fetch_articles_from_naver_api(session, cfg, keyword):
            url = article_api_data.get("link")
            if not url or url in processed_urls: continue
            processed_urls.add(url)
            
            result = process_article(session, cfg, article_api_data)
            if result:
                send_to_backend(session, cfg, result["payload"])
        time.sleep(1)

    print("\n--- âœ… ìˆ˜ì§‘ ì™„ë£Œ (2ë‹¨ê³„ ì¢…ë£Œ) ---")

# ====================================================================
# ğŸš€ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸(main)
# ====================================================================
def main():
    DEFAULTS = Config(
        keywords=["ê²½ì œ", "ì‚¬íšŒ", "IT"],
        max_articles_per_keyword=5,
        total_phases=2,
        backend_endpoint="http://localhost:8080/api/articles/v2",
        timeout=300, wait_min=10, wait_max=20, sleep_min=1.0, sleep_max=3.0,
        user_agent="Mozilla/5.0", accept_language="ko-KR,ko;q=0.9",
        referer="https://news.naver.com/", max_retries=2,
        loop=False, dry_run=False, debug_save_page=False, article_url=None,
    )

    parser = argparse.ArgumentParser(description="Naver News API-based Crawler")
    parser.add_argument("--keywords", nargs="+", default=DEFAULTS.keywords, help="ì´ˆê¸° ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡")
    parser.add_argument("--max-articles-per-keyword", type=int, default=DEFAULTS.max_articles_per_keyword)
    parser.add_argument("--total-phases", type=int, default=DEFAULTS.total_phases, help="í‚¤ì›Œë“œ í™•ì¥ ìˆ˜ì§‘ ë‹¨ê³„ (1 ë˜ëŠ” 2)")
    parser.add_argument("--backend-endpoint", default=DEFAULTS.backend_endpoint)
    parser.add_argument("--dry-run", action="store_true")
    parser.add_argument("--loop", action="store_true", help="ì£¼ê¸°ì ìœ¼ë¡œ ë°˜ë³µ ìˆ˜ì§‘")
    parser.add_argument("--article-url", default=DEFAULTS.article_url, help="ë‹¨ì¼ ê¸°ì‚¬ URL ì§ì ‘ ì²˜ë¦¬")
    parser.add_argument("--wait-min", type=int, default=DEFAULTS.wait_min)
    parser.add_argument("--wait-max", type=int, default=DEFAULTS.wait_max)

    args = parser.parse_args()
    
    cli_args = {k: v for k, v in vars(args).items() if v is not None}
    if not args.loop: #
        cli_args['loop'] = DEFAULTS.loop
    cfg = replace(DEFAULTS, **cli_args)
    
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        print("ğŸš¨ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. NAVER_CLIENT_ID, NAVER_CLIENT_SECRET í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

    session = create_session(cfg)

    # 2ë‹¨ê³„ ìˆ˜ì§‘ ì‹œì—ë§Œ í‚¤ì›Œë“œ ì¶”ì¶œê¸° ë¡œë“œ (ëª¨ë¸ ë¡œë”©ì´ ì˜¤ë˜ ê±¸ë¦¼)
    keyword_extractor = None
    if cfg.total_phases > 1:
        if NewsKeywordExtractor:
            print("ğŸ”„ 2ë‹¨ê³„ ìˆ˜ì§‘ì„ ìœ„í•´ í‚¤ì›Œë“œ ì¶”ì¶œê¸°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
            keyword_extractor = NewsKeywordExtractor()
        else:
            print("âš ï¸ 2ë‹¨ê³„ ìˆ˜ì§‘ì„ ìœ„í•´ 'news_keyword_extractor'ê°€ í•„ìš”í•˜ì§€ë§Œ, ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. 1ë‹¨ê³„ ìˆ˜ì§‘ë§Œ ì§„í–‰í•©ë‹ˆë‹¤.")
            cfg = replace(cfg, total_phases=1)
        
    def crawl_job():
        if cfg.article_url:
            print("â–¶ ë‹¨ì¼ ê¸°ì‚¬ ëª¨ë“œ ì‹¤í–‰")
            result = process_article(session, cfg, {"originallink": cfg.article_url})
            if result:
                send_to_backend(session, cfg, result["payload"])
        else:
            crawl_with_api_2_phase(session, cfg, keyword_extractor)

    if not cfg.loop:
        crawl_job()
    else:
        try:
            while True:
                crawl_job()
                wait = random.randint(cfg.wait_min, cfg.wait_max)
                print(f"â³ ë‹¤ìŒ ì£¼ê¸°ê¹Œì§€ {wait}ì´ˆ ëŒ€ê¸°...")
                time.sleep(wait)
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
